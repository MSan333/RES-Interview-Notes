# 推荐系统 百面百搭





- [推荐系统 百面百搭](#推荐系统-百面百搭)
  - [一、推荐系统导论篇](#一推荐系统导论篇)
  - [二、推荐系统机器学习篇](#二推荐系统机器学习篇)
    - [2.1 【关于 协同过滤篇】那些你不知道的事](#21-关于-协同过滤篇那些你不知道的事)
    - [2.2【关于 矩阵分解篇】那些你不知道的事](#22关于-矩阵分解篇那些你不知道的事)
    - [2.3 【关于 逻辑回归篇】 那些你不知道的事](#23-关于-逻辑回归篇-那些你不知道的事)
    - [2.4 FM 算法篇](#24-fm-算法篇)
    - [2.5 FFM 算法篇](#25-ffm-算法篇)
    - [2.6 GBDT+LR 篇](#26-gbdtlr-篇)
  - [三、推荐系统 深度学习篇](#三推荐系统-深度学习篇)
    - [3.1 AutoRec 篇](#31-autorec-篇)
    - [3.2 NeuralCF模型 篇](#32-neuralcf模型-篇)
    - [3.3 Deep Crossing模型 篇](#33-deep-crossing模型-篇)
    - [3.4 Wide＆Deep模型 篇](#34-widedeep模型-篇)
    - [3.5 FM与深度学习模型的结合 篇](#35-fm与深度学习模型的结合-篇)
  - [四、推荐系统 落地篇](#四推荐系统-落地篇)
  - [五、多角度审视推荐系统篇](#五多角度审视推荐系统篇)
  - [六、推荐系统 评估方法篇](#六推荐系统-评估方法篇)
  - [七、推荐系统 工程落地篇](#七推荐系统-工程落地篇)

## [一、推荐系统导论篇](introduction/)

- 1.1 什么是推荐系统？
- 1.2 推荐系统的作用？
- 1.3 推荐系统的意义？
- 1.4 推荐系统要解决的问题？
- 1.5 常用的推荐系统的逻辑框架是怎么样的呢？
- 1.6 常用的推荐系统的技术架构是怎么样的呢？
- 1.7 推荐系统算法工程师日常解决问题？
- 1.8 推荐系统算法工程师 处理的数据部分有哪些，最后得到什么数据？
- 1.9 推荐系统算法工程师 处理的模型部分有哪些，最后得到什么数据？
- 1.10 模型训练的方式？
- 1.11 推荐系统 的 流程是什么？
- 1.12 推荐系统 的 流程是什么？
- 1.13 推荐系统 与 搜索、广告 的 异同？
- 1.14 推荐系统 整体架构？

### 1.1 什么是推荐系统？

推荐系统是一种信息过滤系统，旨在根据用户的兴趣、行为和偏好，向用户推荐个性化的产品、服务或信息。其主要目的是在大量信息中，帮助用户快速找到他们可能感兴趣的内容。

### 1.2 推荐系统的作用

1. 提高用户体验：通过个性化推荐，帮助用户快速找到感兴趣的内容，提升用户满意度。
2. 增加用户粘性：通过推荐相关内容，增加用户在平台上的停留时间和互动频率。
3. 提高转化率：通过推荐用户可能感兴趣的商品或服务，提高购买或使用的转化率。
4. 优化资源分配：通过推荐系统，可以更有效地分配资源，降低信息过载的风险。

### 1.3 推荐系统的意义

1. 个性化服务：为每个用户提供定制化的内容，满足个性化需求。
2. 信息过滤：在海量信息中筛选出用户可能感兴趣的内容，减少信息过载。
3. 商业价值：通过推荐系统，可以提高产品的曝光率和销售额，增加企业收益。
4. 数据挖掘：通过分析用户行为数据，挖掘用户偏好和兴趣，优化产品和服务。

### 1.4 推荐系统要解决的问题

1. 数据稀疏性：用户和物品的交互数据通常是稀疏的，如何在稀疏数据下进行有效推荐是一个挑战。
2. 冷启动问题：新用户或新物品缺乏历史数据，如何进行推荐是一个难题。
3. 多样性与新颖性：在保证推荐准确性的同时，还需要考虑推荐内容的多样性和新颖性。
4. 实时性：如何在短时间内处理大量数据，生成实时推荐结果。
5. 用户隐私：在收集和使用用户数据时，如何保护用户隐私。

### 1.5 常用的推荐系统的逻辑框架

1. 数据收集：收集用户行为数据、物品数据和上下文数据。
2. 数据处理：对收集的数据进行清洗、预处理和特征提取。
3. 建模：使用机器学习或深度学习算法构建推荐模型。
4. 推荐生成：根据推荐模型生成个性化推荐列表。
5. 评估与优化：通过离线评估和在线实验，评估推荐效果并优化模型。

### 1.6 常用的推荐系统的技术架构

1. 数据层：包括数据源、数据存储和数据处理，负责收集和存储用户行为数据、物品数据等。
2. 算法层：包括特征工程、模型训练和模型预测，负责构建和优化推荐模型。
3. 服务层：包括推荐引擎、推荐策略和推荐结果展示，负责生成个性化推荐列表并展示给用户。
4. 评估层：包括离线评估和在线评估，负责评估推荐系统的效果并进行优化。

### 1.7 推荐系统算法工程师日常解决问题

1. 数据处理：清洗和预处理用户行为数据、物品数据等。
2. 特征工程：提取和构造有效的特征用于模型训练。
3. 模型训练：选择和训练合适的推荐算法模型。
4. 模型评估：评估模型的效果，并进行调优和改进。
5. 实时推荐：优化推荐系统的实时性，确保推荐结果的及时性和准确性。
6. 用户反馈：分析用户反馈，改进推荐策略和模型。

### 1.8 推荐系统算法工程师 处理的数据部分

1. 用户行为数据：用户的点击、浏览、购买、评分等行为数据。
2. 物品数据：物品的属性、类别、标签等信息。
3. 上下文数据：用户的地理位置、时间、设备等上下文信息。

最终得到的数据是：经过清洗和预处理后的特征数据，用于模型训练和预测。

### 1.9 推荐系统算法工程师 处理的模型部分

1. 特征工程：构造用户特征、物品特征和上下文特征。
2. 模型选择：选择合适的推荐算法模型，如协同过滤、矩阵分解、深度学习等。
3. 模型训练：使用训练数据训练推荐模型。
4. 模型评估：评估模型的效果，调整参数和优化模型。

最终得到的数据是：训练好的推荐模型，用于生成个性化推荐结果。

### 1.10 模型训练的方式

1. 离线训练：使用历史数据进行模型训练和评估，通常用于批量处理和模型初始训练。
2. 在线训练：实时更新模型参数，适应用户行为的变化，通常用于实时推荐场景。
3. 增量训练：在已有模型的基础上，逐步更新模型参数，适应新增数据。

### 1.11 推荐系统 的 流程

1. 数据收集：收集用户行为数据、物品数据和上下文数据。
2. 数据处理：对收集的数据进行清洗、预处理和特征提取。
3. 建模：使用机器学习或深度学习算法构建推荐模型。
4. 推荐生成：根据推荐模型生成个性化推荐列表。
5. 评估与优化：通过离线评估和在线实验，评估推荐效果并优化模型。

### 1.12 推荐系统 与 搜索、广告 的 异同

#### 相同点

1. 数据驱动：推荐系统、搜索和广告都依赖于用户行为数据和物品数据进行个性化推荐或展示。
2. 目标相似：都旨在提高用户体验、增加用户粘性和转化率。

#### 不同点

1. 触发方式：推荐系统是主动推荐，用户无需明确表达需求；搜索是用户主动查询，用户明确表达需求；广告是通过竞价等方式展示给用户。
2. 实现方式：推荐系统主要依赖于用户行为和兴趣建模；搜索主要依赖于关键词匹配和相关性排序；广告主要依赖于竞价和用户画像。
3. 评估指标：推荐系统主要关注点击率、转化率等；搜索主要关注查询的相关性和精确度；广告主要关注点击率、转化率和广告收益。

### 1.13 推荐系统 整体架构

1. 数据层：包括数据源、数据存储和数据处理，负责收集和存储用户行为数据、物品数据等。
2. 算法层：包括特征工程、模型训练和模型预测，负责构建和优化推荐模型。
3. 服务层：包括推荐引擎、推荐策略和推荐结果展示，负责生成个性化推荐列表并展示给用户。
4. 评估层：包括离线评估和在线评估，负责评估推荐系统的效果并进行优化。
5. 用户反馈层：收集用户反馈，分析用户行为和偏好，改进推荐策略和模型。

## [二、推荐系统机器学习篇](traditional_recommendation_model/)

### [2.1 【关于 协同过滤篇】那些你不知道的事](https://articles.zsxq.com/id_lje4bgibeb4i.html)

- 一、基础篇
  - 1.1 什么是协同过滤？
  - 1.2 协同过滤的推荐流程是怎么样？
- 二、基于用户的协同过滤 （User-CF-Based）篇
  - 2.1 基于用户的协同过滤 （User-CF-Based） 是什么？
  - 2.2 基于用户的协同过滤 （User-CF-Based） 的思想是什么？
  - 2.3 基于用户的协同过滤 （User-CF-Based） 的特点是什么？
- 三、基于物品的协同过滤 （Item-CF-Based）篇
  - 3.1 基于物品的协同过滤 （Item-CF-Based） 是什么？
  - 3.2 基于物品的协同过滤 （Item-CF-Based） 的思想是什么？
  - 3.3 基于物品的协同过滤 （Item-CF-Based） 的特点是什么？
  - 3.4 基于物品的协同过滤 （Item-CF-Based） 的具体步骤是什么？
- 四、User-CF-Based 与 Item-CF-Based 对比篇
  - 4.1 User-CF-Based 与 Item-CF-Based 的应用场景的区别
  - 4.2 User-CF-Based 与 Item-CF-Based 的存在问题的区别
- 五、User-CF-Based 与 Item-CF-Based 问题篇

  

>
>### 一、基础篇

#### 1.1 什么是协同过滤？

协同过滤（Collaborative Filtering, CF）是一种基于用户行为数据的推荐方法，通过分析用户的历史行为数据（如评分、点击、购买等），找到相似的用户或物品，从而进行个性化推荐。协同过滤主要分为基于用户的协同过滤（User-CF）和基于物品的协同过滤（Item-CF）。

#### 1.2 协同过滤的推荐流程

1. 数据收集：收集用户对物品的行为数据，如评分、点击、购买等。
2. 相似度计算：计算用户之间或物品之间的相似度。
3. 邻居选择：根据相似度选择与目标用户或物品相似的用户或物品。
4. 评分预测：根据相似用户或物品的行为，预测目标用户对未见过物品的评分。
5. 推荐生成：根据预测评分生成推荐列表，推荐给用户。

### 二、基于用户的协同过滤（User-CF-Based）篇

#### 2.1 基于用户的协同过滤（User-CF-Based）是什么？

基于用户的协同过滤（User-CF-Based）是一种通过分析用户之间的相似性，找到与目标用户兴趣相似的用户，从而推荐这些相似用户喜欢的物品给目标用户的推荐方法。

#### 2.2 基于用户的协同过滤（User-CF-Based）的思想是什么？

基于用户的协同过滤的核心思想是“物以类聚，人以群分”，即如果两个用户在过去对物品的评价或行为相似，那么他们在未来对物品的喜好也可能相似。因此，可以通过找到与目标用户相似的用户，将这些用户喜欢的物品推荐给目标用户。

#### 2.3 基于用户的协同过滤（User-CF-Based）的特点是什么？

1. 简单直观：基于用户的协同过滤方法简单易懂，容易实现。
2. 依赖用户行为数据：需要大量的用户行为数据来计算用户之间的相似性。
3. 适用于用户活跃度高的场景：在用户行为数据丰富的情况下，推荐效果较好。
4. 冷启动问题：对于新用户或新物品，缺乏历史行为数据，推荐效果较差。

### 三、基于物品的协同过滤（Item-CF-Based）篇

#### 3.1 基于物品的协同过滤（Item-CF-Based）是什么？

基于物品的协同过滤（Item-CF-Based）是一种通过分析物品之间的相似性，找到与目标物品相似的物品，从而推荐这些相似物品给用户的推荐方法。

#### 3.2 基于物品的协同过滤（Item-CF-Based）的思想是什么？

基于物品的协同过滤的核心思想是“物以类聚”，即如果两个物品在过去被同一批用户喜欢或评价相似，那么它们在未来也可能被同一批用户喜欢。因此，可以通过找到与目标物品相似的物品，将这些物品推荐给用户。

#### 3.3 基于物品的协同过滤（Item-CF-Based）的特点是什么？

1. 稳定性较好：物品的相似性相对稳定，不易受个别用户行为的影响。
2. 依赖物品行为数据：需要大量的物品行为数据来计算物品之间的相似性。
3. 适用于物品数量多的场景：在物品数量丰富的情况下，推荐效果较好。
4. 冷启动问题：对于新物品，缺乏历史行为数据，推荐效果较差。

#### 3.4 基于物品的协同过滤（Item-CF-Based）的具体步骤是什么？

1. 数据收集：收集用户对物品的行为数据，如评分、点击、购买等。
2. 相似度计算：计算物品之间的相似度。
3. 邻居选择：根据相似度选择与目标物品相似的物品。
4. 评分预测：根据相似物品的行为，预测用户对未见过物品的评分。
5. 推荐生成：根据预测评分生成推荐列表，推荐给用户。

### 四、User-CF-Based 与 Item-CF-Based 对比篇

#### 4.1 User-CF-Based 与 Item-CF-Based 的应用场景的区别

1. User-CF-Based：适用于用户行为数据丰富、用户活跃度高的场景，如社交网络、视频推荐等。
2. Item-CF-Based：适用于物品数量多、物品之间相似性稳定的场景，如电商平台、书籍推荐等。

#### 4.2 User-CF-Based 与 Item-CF-Based 的存在问题的区别

1. User-CF-Based：
   - 数据稀疏性：用户行为数据稀疏，导致相似度计算困难。
   - 冷启动问题：新用户缺乏历史行为数据，推荐效果较差。

2. Item-CF-Based：
   - 数据稀疏性：物品行为数据稀疏，导致相似度计算困难。
   - 冷启动问题：新物品缺乏历史行为数据，推荐效果较差。

### 五、User-CF-Based 与 Item-CF-Based 问题篇

1. 数据稀疏性：如何在数据稀疏的情况下计算用户或物品之间的相似性。
2. 冷启动问题：如何解决新用户或新物品缺乏历史行为数据的问题。
3. 计算复杂度：如何提高相似度计算和推荐生成的效率，处理大规模数据。
4. 多样性与新颖性：如何在保证推荐准确性的同时，增加推荐内容的多样性和新颖性。
5. 用户隐私：如何在收集和使用用户数据时，保护用户隐私。

### [2.2【关于 矩阵分解篇】那些你不知道的事](https://articles.zsxq.com/id_4hjo78at5lj8.html)

- 一、动机篇
  - 1.1 为什么 需要 矩阵分解？
- 二、隐语义模型 介绍篇
  - 2.1 什么是 隐语义模型？
  - 2.2 隐语义模型 存在什么问题？
- 三、矩阵分解 介绍篇
  - 3.1 如何 获取 ⽤户矩阵Q 和 音乐矩阵P？
  - 3.2 矩阵分解 思路 是什么？
  - 3.3 矩阵分解 原理 是什么？
  - 3.4 如何 利用 矩阵分解 计算 用户 u 对 物品 v 的 评分？
- 四、矩阵分解 优缺点篇
  - 4.1 矩阵分解 存在什么问题？

>
### 一、动机篇

#### 1.1 为什么需要矩阵分解？

矩阵分解是一种处理推荐系统中数据稀疏性和高维性问题的有效方法。传统的协同过滤方法在面对大量用户和物品时，往往会遇到计算复杂度高和数据稀疏的问题。矩阵分解通过将用户-物品交互矩阵分解为低维的用户矩阵和物品矩阵，能够有效地捕捉潜在的用户兴趣和物品特征，从而提高推荐的准确性和效率。

### 二、隐语义模型介绍篇

#### 2.1 什么是隐语义模型？

隐语义模型（Latent Semantic Model）是通过识别和利用数据中的潜在结构来进行推荐的一种模型。它通过将用户和物品映射到一个共同的潜在特征空间中，来捕捉用户的兴趣和物品的特征。矩阵分解技术，如奇异值分解（SVD）和非负矩阵分解（NMF），是实现隐语义模型的常用方法。

#### 2.2 隐语义模型存在什么问题？

1. 数据稀疏性：在用户-物品交互矩阵中，评分数据通常是稀疏的，这会影响模型的准确性。
2. 计算复杂度：对于大规模数据集，矩阵分解的计算复杂度较高，可能需要较长的训练时间。
3. 冷启动问题：对于新用户或新物品，缺乏历史数据，模型难以进行准确预测。
4. 过拟合风险：如果模型过于复杂，可能会过拟合训练数据，影响泛化能力。

### 三、矩阵分解介绍篇

#### 3.1 如何获取用户矩阵Q和物品矩阵P？

用户矩阵 \( Q \) 和物品矩阵 \( P \) 可以通过矩阵分解方法从用户-物品交互矩阵 \( R \) 中获取。通常使用优化算法（如梯度下降）最小化预测评分与实际评分之间的误差，来学习出用户和物品的潜在特征矩阵。

#### 3.2 矩阵分解思路是什么？

矩阵分解的基本思路是将用户-物品交互矩阵 \( R \) 分解为两个低维矩阵 \( Q \) 和 \( P \)，即：

$$ R \approx Q \cdot P^T $$

其中，\( Q \) 是用户特征矩阵，\( P \) 是物品特征矩阵。通过优化这些矩阵，使得它们的乘积能够很好地近似原始的交互矩阵。

#### 3.3 矩阵分解原理是什么？

矩阵分解通过将高维的用户-物品交互矩阵投影到低维的潜在特征空间中，来捕捉用户和物品的隐含特征。通过最小化预测评分与实际评分的误差，可以优化用户和物品的特征表示，使得它们能够准确地反映用户的偏好和物品的特征。

#### 3.4 如何利用矩阵分解计算用户 \( u \) 对物品 \( v \) 的评分？

用户 \( u \) 对物品 \( v \) 的预测评分可以通过用户特征向量 \( q_u \) 和物品特征向量 \( p_v \) 的内积来计算：

$$ \hat{r}_{uv} = q_u \cdot p_v^T $$

其中，\( \hat{r}_{uv} \) 是预测评分，\( q_u \) 是用户 \( u \) 的特征向量，\( p_v \) 是物品 \( v \) 的特征向量。

### 四、矩阵分解优缺点篇

#### 4.1 矩阵分解存在什么问题？

1. 数据稀疏性：在数据稀疏的情况下，矩阵分解可能无法准确捕捉用户和物品的特征。
2. 冷启动问题：对于新用户或新物品，缺乏历史数据，难以进行准确预测。
3. 计算复杂度：对于大规模数据集，矩阵分解的计算复杂度较高，可能需要较长的训练时间。
4. 过拟合风险：如果模型过于复杂，可能会过拟合训练数据，影响泛化能力。
5. 难以解释：矩阵分解得到的潜在特征通常缺乏直接的可解释性，不易理解。

### [2.3 【关于 逻辑回归篇】 那些你不知道的事](https://articles.zsxq.com/id_3kstrwlvfuw0.html)

- 一、动机篇
  - 1.1 为什么 需要 逻辑回归？
- 二、逻辑回归 介绍篇
  - 2.1 逻辑回归 如何解决 上述问题？
  - 2.2 什么是逻辑回归
- 三、逻辑回归 推导篇
  - 3.1 逻辑回归 如何推导？
  - 3.2 逻辑回归 如何求解优化？
- 四、逻辑回归 推荐流程篇
  - 4.1 逻辑回归 推荐流程？
- 五、逻辑回归 优缺点篇
  - 5.1 逻辑回归 有哪些优点？
  - 5.2 逻辑回归 有哪些缺点？

>
### 一、动机篇

#### 1.1 为什么需要逻辑回归？

逻辑回归是一种用于分类问题的统计模型，主要用于解决二分类问题。它被广泛应用于推荐系统、信用评分、医疗诊断等领域。逻辑回归的动机在于它能够提供一种简单而有效的方法来处理分类问题，特别是在需要解释性和线性可分的情况下。

### 二、逻辑回归介绍篇

#### 2.1 逻辑回归如何解决上述问题？

逻辑回归通过将线性回归的输出通过一个逻辑函数（sigmoid函数）映射到(0,1)之间，从而将输出解释为概率。这使得逻辑回归能够处理二分类问题，并且能够自然地处理概率输出。

#### 2.2 什么是逻辑回归？

逻辑回归是一种用于二分类问题的广义线性模型。它假设输出变量 \( y \) 是一个二元变量，输入变量 \( X \) 是一个特征向量。逻辑回归通过学习权重向量 \( \beta \)，使得线性组合 \( X \cdot \beta \) 经过sigmoid函数后，能够逼近 \( y \) 的概率：

$$ P(y = 1 | X) = \frac{1}{1 + e^{-X \cdot \beta}} $$

### 三、逻辑回归推导篇

#### 3.1 逻辑回归如何推导？

逻辑回归的推导基于对数几率（log-odds）和sigmoid函数。通过假设对数几率是输入特征的线性组合：

$$ \log\left(\frac{P(y=1|X)}{1-P(y=1|X)}\right) = X \cdot \beta $$

通过对数几率的逆变换，可以得到逻辑回归的概率输出形式。

#### 3.2 逻辑回归如何求解优化？

逻辑回归的优化问题通常通过最大化似然函数（Maximum Likelihood Estimation, MLE）来求解。具体来说，是通过最大化以下对数似然函数：

$$ \mathcal{L}(\beta) = \sum_{i=1}^{n} \left[ y_i \log(P(y_i=1|X_i)) + (1-y_i) \log(1-P(y_i=1|X_i)) \right] $$

常用的优化算法包括梯度下降、牛顿法和拟牛顿法（如L-BFGS）。

### 四、逻辑回归推荐流程篇

#### 4.1 逻辑回归推荐流程？

1. 数据准备：收集并预处理数据，包括特征选择和特征工程。
2. 模型训练：使用训练数据集训练逻辑回归模型，学习权重参数。
3. 模型验证：使用验证数据集评估模型性能，调整参数以优化模型。
4. 模型预测：使用训练好的模型对新数据进行预测，输出概率值。
5. 推荐生成：根据预测概率生成推荐列表，推荐给用户。

### 五、逻辑回归优缺点篇

#### 5.1 逻辑回归有哪些优点？

1. 简单易用：逻辑回归模型简单，易于实现和解释。
2. 可解释性强：模型输出可以解释为概率，权重参数可以解释为特征的影响力。
3. 计算效率高：逻辑回归的计算复杂度低，适合大规模数据集。
4. 鲁棒性好：对噪声数据不敏感，具有较好的稳健性。

#### 5.2 逻辑回归有哪些缺点？

1. 线性假设：逻辑回归假设特征与输出之间是线性关系，无法捕捉复杂的非线性关系。
2. 高维数据问题：在高维数据中，逻辑回归可能会出现过拟合问题。
3. 二分类限制：逻辑回归主要用于二分类问题，多分类问题需要扩展（如softmax回归）。
4. 特征独立性假设：逻辑回归假设特征之间是独立的，可能不适用于特征相关性强的情况。

### [2.4 FM 算法篇](https://articles.zsxq.com/id_4zqld440t2lm.html)

- 一、为什么要使用 FM？
- 二、FM 的思路是什么？
- 三、FM 的优点？
- 四、FM 的缺点？
- 五、POLY2 vs FM？

>
### 一、为什么要使用 FM？

因子分解机（Factorization Machines, FM）是一种广泛应用于推荐系统和预测任务的模型，特别适用于处理高维稀疏数据。使用FM的主要原因包括：

1. **捕捉特征交互**：FM能够自动捕捉特征之间的二阶交互作用，适用于高维稀疏数据场景。
2. **高效计算**：FM通过因子分解技术，能够高效地计算特征交互，避免了显式构造交互特征的计算复杂度。
3. **处理稀疏数据**：FM在处理稀疏数据时表现出色，可以有效地处理缺失值和稀疏矩阵。
4. **泛化能力强**：FM通过隐向量表示特征交互，能够在数据稀疏的情况下仍然保持较好的泛化能力。

### 二、FM 的思路是什么？

FM的基本思路是通过引入隐向量来表示特征之间的交互作用。具体来说，FM模型将输入特征向量 \( \mathbf{x} \) 表示为：

$$ \hat{y} = w_0 + \sum_{i=1}^{n} w_i x_i + \sum_{i=1}^{n} \sum_{j=i+1}^{n} \langle \mathbf{v}_i, \mathbf{v}_j \rangle x_i x_j $$

其中：
- \( w_0 \) 是全局偏置项。
- \( w_i \) 是特征 \( i \) 的权重。
- \( \mathbf{v}_i \) 是特征 \( i \) 的隐向量，表示特征 \( i \) 在隐空间中的表示。
- \( \langle \mathbf{v}_i, \mathbf{v}_j \rangle \) 表示特征 \( i \) 和特征 \( j \) 的隐向量的内积，捕捉特征之间的交互作用。

### 三、FM 的优点

1. **自动特征交互**：FM能够自动学习特征之间的二阶交互作用，无需显式构造交互特征。
2. **处理稀疏数据**：FM在处理高维稀疏数据时表现出色，能够有效应对数据稀疏性问题。
3. **高效计算**：通过因子分解技术，FM能够高效地计算特征交互，计算复杂度低。
4. **泛化能力强**：FM通过隐向量表示特征交互，能够在数据稀疏的情况下保持较好的泛化能力。
5. **灵活性高**：FM可以与其他模型结合使用，如与深度学习模型结合，进一步提升性能。

### 四、FM 的缺点

1. **仅捕捉二阶交互**：FM主要捕捉二阶特征交互，对于高阶特征交互需要扩展模型（如高阶因子分解机）。
2. **模型复杂度**：虽然FM计算高效，但模型训练过程中隐向量的学习仍然需要一定的计算资源。
3. **特征解释性差**：隐向量的表示较难直接解释特征之间的交互关系。

### 五、POLY2 vs FM

#### POLY2

POLY2是一个多项式回归模型，能够捕捉特征之间的二阶交互作用。其基本形式为：

$$ \hat{y} = w_0 + \sum_{i=1}^{n} w_i x_i + \sum_{i=1}^{n} \sum_{j=i+1}^{n} w_{ij} x_i x_j $$

其中，\( w_{ij} \) 是特征 \( i \) 和特征 \( j \) 的交互权重。

#### FM vs POLY2

1. **特征交互表示**：
   - POLY2：显式地为每对特征交互分配一个权重 \( w_{ij} \)，参数数量随特征数量的平方增长，容易过拟合。
   - FM：通过隐向量表示特征交互，参数数量随特征数量线性增长，避免了显式构造交互特征的计算复杂度。

2. **计算复杂度**：
   - POLY2：计算复杂度较高，尤其在高维特征情况下，参数数量庞大，训练和预测效率低。
   - FM：计算复杂度低，通过因子分解技术高效计算特征交互，适用于高维稀疏数据。

3. **泛化能力**：
   - POLY2：容易过拟合，尤其在数据稀疏的情况下，泛化能力较差。
   - FM：通过隐向量表示特征交互，能够在数据稀疏的情况下保持较好的泛化能力。

4. **灵活性**：
   - POLY2：模型较为简单，适用于特征数量较少的场景。
   - FM：模型灵活性高，可以与其他模型结合使用，进一步提升性能。

综上所述，FM在处理高维稀疏数据、捕捉特征交互和提高泛化能力方面表现更为优越，而POLY2适用于特征数量较少、数据密集的场景。

### [2.5 FFM 算法篇](https://articles.zsxq.com/id_uz5p2ategto9.html)

- 一、为什么要使用 FFM？
- 二、FFM 的思路是什么？
- 三、FM vs FFM？

>
### 一、为什么要使用 FFM？

字段感知因子分解机（Field-aware Factorization Machine, FFM）是在因子分解机（FM）的基础上进一步改进的一种模型，特别适用于处理具有字段（Field）信息的高维稀疏数据。使用FFM的主要原因包括：

1. **捕捉字段间的特征交互**：FFM能够更细粒度地捕捉特征之间的交互作用，尤其是不同字段之间的交互。
2. **提升预测性能**：在具有字段信息的数据集上，FFM通常能够比FM取得更好的预测效果。
3. **处理复杂数据结构**：FFM适用于更复杂的数据结构，如广告点击率预测、推荐系统等领域。

### 二、FFM 的思路是什么？

FFM的基本思路是通过引入字段感知的隐向量来表示特征之间的交互作用。具体来说，FFM模型将输入特征向量 \( \mathbf{x} \) 表示为：

$$ \hat{y} = w_0 + \sum_{i=1}^{n} w_i x_i + \sum_{i=1}^{n} \sum_{j=i+1}^{n} \langle \mathbf{v}_{i, f_j}, \mathbf{v}_{j, f_i} \rangle x_i x_j $$

其中：
- \( w_0 \) 是全局偏置项。
- \( w_i \) 是特征 \( i \) 的权重。
- \( \mathbf{v}_{i, f_j} \) 是特征 \( i \) 在字段 \( f_j \) 中的隐向量，表示特征 \( i \) 在字段 \( f_j \) 中的表示。
- \( \langle \mathbf{v}_{i, f_j}, \mathbf{v}_{j, f_i} \rangle \) 表示特征 \( i \) 在字段 \( f_j \) 中的隐向量与特征 \( j \) 在字段 \( f_i \) 中的隐向量的内积，捕捉特征之间的交互作用。

### 三、FM vs FFM

#### 模型结构

- **FM**：在FM中，每个特征都有一个固定的隐向量来表示其与其他特征的交互作用。公式为：
  $$ \hat{y} = w_0 + \sum_{i=1}^{n} w_i x_i + \sum_{i=1}^{n} \sum_{j=i+1}^{n} \langle \mathbf{v}_i, \mathbf{v}_j \rangle x_i x_j $$
  
- **FFM**：在FFM中，每个特征在每个字段中都有一个独立的隐向量来表示其与其他字段中特征的交互作用。公式为：
  $$ \hat{y} = w_0 + \sum_{i=1}^{n} w_i x_i + \sum_{i=1}^{n} \sum_{j=i+1}^{n} \langle \mathbf{v}_{i, f_j}, \mathbf{v}_{j, f_i} \rangle x_i x_j $$

#### 特征交互

- **FM**：FM捕捉的是特征之间的二阶交互作用，不考虑字段信息。
- **FFM**：FFM捕捉的是字段之间的特征交互作用，更细粒度地考虑了不同字段之间的交互。

#### 参数数量

- **FM**：每个特征只有一个隐向量，参数数量相对较少。
- **FFM**：每个特征在每个字段中都有一个隐向量，参数数量较多，模型更复杂。

#### 计算复杂度

- **FM**：计算复杂度较低，适合处理高维稀疏数据。
- **FFM**：计算复杂度较高，由于每个特征在每个字段中都有一个隐向量，训练和预测过程需要更多的计算资源。

#### 泛化能力

- **FM**：在数据稀疏的情况下，FM能够保持较好的泛化能力。
- **FFM**：在具有字段信息的数据集上，FFM通常能够比FM取得更好的泛化能力和预测效果。

#### 应用场景

- **FM**：适用于处理高维稀疏数据，特别是在推荐系统、广告点击率预测等领域表现良好。
- **FFM**：适用于具有字段信息的复杂数据结构，特别是在广告点击率预测、推荐系统等领域表现更为优越。

综上所述，FFM通过引入字段感知的隐向量，能够更细粒度地捕捉特征之间的交互作用，提升预测性能。然而，FFM的计算复杂度和参数数量较高，需要更多的计算资源和训练时间。FM则在处理高维稀疏数据时表现良好，计算效率较高。选择使用哪种模型应根据具体的应用场景和数据特点来决定。

### [2.6 GBDT+LR 篇](https://articles.zsxq.com/id_0l5gdn0wjtsp.html)

- 一、动机篇
  - 1.1 为什么 需要 GBDT+LR？
- 二、GBDT 介绍篇
  - 2.1 GBDT 的基础结构是什么样的？
  - 2.2 GBDT 的学习方式？
  - 2.3 GBDT 的思路？
  - 2.4 GBDT 的特点是怎么样？
  - 2.5 GBDT 所用分类器是什么？
  - 2.6 GBDT 解决二分类和回归问题的方式？
  - 2.7 GBDT 损失函数 是什么？
  - 2.8 构建分类GBDT的步骤 是什么？
  - 2.9 GBDT 优缺点篇？
- 三、GBDT+LR 模型介绍篇
  - 3.1 GBDT+LR 模型 思路是什么样？
  - 3.2 GBDT+LR 模型 步骤是什么样？
  - 3.3 GBDT+LR 模型 关键点是什么样？
  - 3.4 GBDT+LR 模型 本质是什么样？
- 四、GBDT+LR 优缺点篇
  - 4.1 GBDT+LR 的优点是什么？
  - 4.2 GBDT+LR 的缺点是什么？
- 五、问题讨论
  - 5.1 为什么要使用集成的决策树模型，而不是单棵的决策树模型？
  - 5.2 为什么建树采用GBDT而非RF？
  - 5.3 Logistic Regression是一个线性分类器，也就是说会忽略掉特征与特征之间的关联信息，那么是否可以采用构建新的交叉特征这一特征组合方式从而提高模型的效果？
  - 5.4 GBDT很有可能构造出的新训练数据是高维的稀疏矩阵，而Logistic Regression使用高维稀疏矩阵进行训练，会直接导致计算量过大，特征权值更新缓慢的问题？
  - 5.5 FM 因为采用FM对本来已经是高维稀疏矩阵做完特征交叉后，新的特征维度会更加多，并且由于元素非0即1，新的特征数据可能也会更加稀疏，那么怎么办？
  - 5.6 为什么要将GBDT与LR融合？

>
### 一、动机篇

#### 1.1 为什么需要 GBDT+LR？

GBDT+LR 是一种结合梯度提升决策树（GBDT）和逻辑回归（LR）的混合模型，广泛用于推荐系统和广告点击率预测等领域。其动机在于利用两者的优势：

1. **特征自动化**：GBDT能够自动捕捉特征之间的非线性关系和交互作用，生成高质量的特征。
2. **线性模型解释性**：LR作为线性模型，具有良好的可解释性和计算效率。
3. **提升模型性能**：通过GBDT生成的特征，LR能够更好地进行分类，提升模型的预测性能。

### 二、GBDT 介绍篇

#### 2.1 GBDT 的基础结构是什么样的？

GBDT（Gradient Boosting Decision Tree）是一种基于决策树的集成学习算法。它通过构建多个决策树（通常是回归树），将它们的预测结果进行加权平均或加权求和，以提升模型的预测性能。每棵树都是在前一棵树的基础上，通过学习残差来构建的。

#### 2.2 GBDT 的学习方式？

GBDT采用梯度提升的学习方式。具体来说，它通过逐步构建树模型来优化损失函数。在每一步中，GBDT计算当前模型的残差，然后通过拟合一个新树来最小化这些残差的梯度。

#### 2.3 GBDT 的思路？

GBDT的核心思路是通过逐步优化损失函数来提升模型性能。它通过构建一系列决策树，每棵树都试图纠正前一棵树的错误预测。最终模型是这些树的加权和。

#### 2.4 GBDT 的特点是怎么样？

1. **强大的非线性建模能力**：GBDT能够捕捉特征之间的复杂非线性关系。
2. **鲁棒性**：对异常值和噪声不敏感。
3. **自动特征选择**：能够自动选择重要特征，减少特征工程的工作量。
4. **支持多种损失函数**：适用于回归、分类等多种任务。

#### 2.5 GBDT 所用分类器是什么？

GBDT通常使用决策树作为基分类器。每棵树通过拟合当前模型的残差，来优化整体模型的损失函数。

#### 2.6 GBDT 解决二分类和回归问题的方式？

- **二分类问题**：GBDT通过最小化对数损失函数来构建模型，输出概率值。
- **回归问题**：GBDT通过最小化平方误差损失函数来构建模型，输出连续值。

#### 2.7 GBDT 损失函数是什么？

GBDT支持多种损失函数，常用的包括：
- **平方误差损失**：用于回归问题。
- **对数损失**：用于二分类问题。
- **指数损失**：用于提升模型的鲁棒性。

#### 2.8 构建分类GBDT的步骤是什么？

1. **初始化模型**：使用常数值初始化模型。
2. **计算残差**：计算当前模型的预测误差（残差）。
3. **拟合新树**：使用残差拟合一个新的决策树。
4. **更新模型**：将新树的输出加到现有模型中。
5. **重复步骤2-4**：直到达到预定的树数量或损失函数收敛。

#### 2.9 GBDT 优缺点篇

**优点**：

1. **强大的建模能力**：能够捕捉复杂的非线性关系。
2. **鲁棒性好**：对异常值和噪声不敏感。
3. **自动特征选择**：能够自动选择重要特征，减少特征工程的工作量。
4. **灵活性高**：支持多种损失函数，适用于多种任务。

**缺点**：

1. **计算复杂度高**：训练时间较长，尤其在大规模数据集上。
2. **参数调优复杂**：需要对多个超参数进行调优，如树的数量、深度等。
3. **对内存要求高**：由于需要存储多个树模型，对内存要求较高。
### GBDT+LR 模型介绍篇

#### 3.1 GBDT+LR 模型思路是什么样？

GBDT+LR模型的基本思路是将GBDT和LR结合起来，利用GBDT的强大特征生成能力和LR的高效线性分类能力。具体来说，GBDT首先通过构建树模型来捕捉特征之间的复杂非线性关系，并生成新的特征表示，然后将这些新的特征输入到LR模型中进行分类。

#### 3.2 GBDT+LR 模型步骤是什么样？

1. **训练GBDT模型**：
   - 使用原始特征训练GBDT模型。
   - GBDT模型通过构建多棵决策树来捕捉特征之间的非线性关系。
2. **生成新的特征**：
   - 利用训练好的GBDT模型对原始数据进行预测，将每棵树的叶子节点编号作为新的特征。
   - 每个样本经过GBDT模型后，会被映射到多个叶子节点，这些叶子节点的编号构成新的特征向量。
3. **训练LR模型**：
   - 使用GBDT生成的新的特征向量作为输入，训练LR模型。
   - LR模型利用这些新的特征进行分类，输出最终的预测结果。

#### 3.3 GBDT+LR 模型关键点是什么样？

1. **特征生成**：GBDT通过构建树模型，自动生成新的特征表示，捕捉特征之间的非线性关系。
2. **特征映射**：将GBDT模型的叶子节点编号作为新的特征，输入到LR模型中。
3. **模型融合**：将GBDT和LR结合起来，利用两者的优势提升模型性能。

#### 3.4 GBDT+LR 模型本质是什么样？

GBDT+LR模型的本质是利用GBDT的特征生成能力和LR的线性分类能力，通过两者的结合，提升模型的预测性能。GBDT捕捉特征之间的复杂非线性关系，而LR利用这些新的特征进行高效的线性分类。

### GBDT+LR 优缺点篇

#### 4.1 GBDT+LR 的优点是什么？

1. **强大的特征生成能力**：GBDT能够自动生成新的特征表示，捕捉特征之间的复杂非线性关系。
2. **高效的线性分类**：LR作为线性分类器，具有良好的可解释性和计算效率。
3. **提升模型性能**：通过GBDT生成的特征，LR能够更好地进行分类，提升模型的预测性能。
4. **灵活性高**：GBDT+LR模型可以适应不同的数据集和任务，具有较高的灵活性。

#### 4.2 GBDT+LR 的缺点是什么？

1. **计算复杂度高**：GBDT的训练过程计算复杂度较高，尤其在大规模数据集上。
2. **特征映射复杂**：GBDT生成的特征需要进行特征映射，可能导致维度较高，增加计算负担。
3. **模型调优复杂**：GBDT和LR都有多个超参数需要调优，增加了模型调优的复杂性。

### 问题讨论

#### 5.1 为什么要使用集成的决策树模型，而不是单棵的决策树模型？

集成的决策树模型（如GBDT）通过构建多个决策树，能够捕捉更多的特征之间的交互和非线性关系，提高模型的泛化能力和预测性能。单棵决策树模型容易过拟合，泛化能力较差。

#### 5.2 为什么建树采用GBDT而非RF？

GBDT通过逐步优化损失函数，能够更好地捕捉特征之间的非线性关系和复杂交互。相比之下，随机森林（RF）是通过构建多棵独立的决策树进行投票，虽然能够提高稳定性，但在捕捉复杂特征交互方面不如GBDT。

#### 5.3 Logistic Regression是一个线性分类器，也就是说会忽略掉特征与特征之间的关联信息，那么是否可以采用构建新的交叉特征这一特征组合方式从而提高模型的效果？

是的，构建新的交叉特征可以提高模型的效果。GBDT+LR模型通过GBDT自动生成新的特征表示，捕捉特征之间的复杂交互，然后将这些新的特征输入到LR模型中进行分类，提升了模型的效果。

#### 5.4 GBDT很有可能构造出的新训练数据是高维的稀疏矩阵，而Logistic Regression使用高维稀疏矩阵进行训练，会直接导致计算量过大，特征权值更新缓慢的问题？

确实，GBDT生成的特征可能是高维稀疏矩阵，使用LR进行训练时可能导致计算量过大和特征权值更新缓慢。可以通过以下方法缓解：
1. **特征选择**：选择重要的特征，减少特征维度。
2. **正则化**：使用正则化技术，防止过拟合并加速训练。
3. **优化算法**：使用高效的优化算法，如L-BFGS，提升训练效率。

#### 5.5 FM 因为采用FM对本来已经是高维稀疏矩阵做完特征交叉后，新的特征维度会更加多，并且由于元素非0即1，新的特征数据可能也会更加稀疏，那么怎么办？

对于高维稀疏矩阵，可以考虑以下方法：
1. **特征选择**：选择重要的特征，减少特征维度。
2. **特征降维**：使用特征降维技术，如PCA，减少特征维度。
3. **高效优化**：使用高效的优化算法，提升训练效率。
4. **分布式计算**：利用分布式计算框架（如Spark），处理大规模数据。

#### 5.6 为什么要将GBDT与LR融合？

将GBDT与LR融合的原因在于：
1. **特征生成**：GBDT能够自动生成新的特征表示，捕捉特征之间的复杂非线性关系。
2. **线性分类**：LR作为线性分类器，具有良好的可解释性和计算效率。
3. **提升性能**：通过GBDT生成的特征，LR能够更好地进行分类，提升模型的预测性能。
4. **灵活性**：GBDT+LR模型可以适应不同的数据集和任务，具有较高的灵活性。


## 三、推荐系统 深度学习篇

### 3.1 AutoRec 篇

- 什么是自编码器?
- AutoRec 思路 是什么？
- AutoRec 基本原理是什么？
- AutoRec模型的结构 长什么样子？
- AutoRec模型的特点？
- AutoRec模型的存在问题？
  
> [点击查看答案](https://articles.zsxq.com/id_dntrd6igjk9i.html)

### 3.2 NeuralCF模型 篇

- 为什么需要NeuralCF模型？
- NeuralCF模型 的 普通结构？
- NeuralCF模型 的 混合结构？
- NeuralCF模型 主要思想？
- NeuralCF模型的优势和局限性？

> [点击查看答案](https://articles.zsxq.com/id_bjd8e1r6kow3.html)

### 3.3 Deep Crossing模型 篇

- 为什么需要 Deep Crossing？
- Deep Crossing 模型的所用特征 是什么？
- Deep Crossing 模型的模型结构？

> [点击查看答案](https://articles.zsxq.com/id_wl94fgqn0r5t.html)

### 3.4 Wide＆Deep模型 篇

- 模型的记忆能力与泛化能力
- Wide＆Deep模型 模型结构？
- Wide＆Deep模型 模型 Trick？
- Wide＆Deep模型 优点是什么？
- Wide＆Deep模型的影响力？
- Wide＆Deep模型的进化——Deep＆Cross模型？

> [点击查看答案](https://articles.zsxq.com/id_di0tp2qalgdx.html)

### 3.5 FM与深度学习模型的结合 篇

- 为什么需要 DeepFM？
- DeepFM 结构 介绍一下？
- DeepFM 思路？
- DeepFM 与 Deep＆Cross模型 异同点？

> [点击查看答案](https://articles.zsxq.com/id_wpbdemx6amp9.html)

## 四、推荐系统 落地篇


## 五、多角度审视推荐系统篇


## 六、推荐系统 评估方法篇

## 七、推荐系统 工程落地篇
